{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_movies = spark.read.csv(\"movies.csv\",header=True,sep=\",\");\n",
    "\n",
    "df_rating = spark.read.csv(\"ratings.csv\",header=True);\n",
    "print(df_movies.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_joint = df_movies.join(df_rating, df_movies.movieId == df_rating.movieId)\\\n",
    ".select(df_movies.movieId,df_movies.genres,df_movies.title,\\\n",
    "        df_rating.userId,df_rating.rating,df_rating.timestamp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joint.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joint = df_joint.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=long(p[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_joint = df_joint.withColumn(\"userId\", df_joint[\"userId\"].cast(IntegerType()))\n",
    "df_joint = df_joint.withColumn(\"movieId\", df_joint[\"movieId\"].cast(IntegerType()))\n",
    "df_joint = df_joint.withColumn(\"rating\", df_joint[\"rating\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joint.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = df_joint.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.1500808446632762\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#lines = spark.read.text(\"data/mllib/als/sample_movielens_ratings.txt\").rdd\n",
    "#parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "#ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    " #                                    rating=float(p[2]), timestamp=long(p[3])))\n",
    "#ratings = spark.createDataFrame(ratingsRDD)\n",
    "#(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = df_joint.select(als.getUserCol()).distinct()\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = df_joint.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   148|[[89904, 6.751604...|\n",
      "|   463|[[161582, 5.93614...|\n",
      "|   471|[[72171, 12.02051...|\n",
      "|   496|[[3266, 7.1788297...|\n",
      "|   243|[[5303, 11.726888...|\n",
      "|   392|[[2318, 8.787418]...|\n",
      "|   540|[[2261, 6.7266893...|\n",
      "|    31|[[65188, 8.068945...|\n",
      "|   516|[[179819, 8.71031...|\n",
      "|    85|[[2398, 6.934878]...|\n",
      "|   137|[[52435, 5.702093...|\n",
      "|   251|[[70946, 11.63048...|\n",
      "|   451|[[52281, 7.258337...|\n",
      "|   580|[[104879, 6.88864...|\n",
      "|    65|[[86345, 6.501846...|\n",
      "|   458|[[131013, 10.2180...|\n",
      "|    53|[[2739, 6.814709]...|\n",
      "|   255|[[5992, 11.773895...|\n",
      "|   481|[[137857, 5.82718...|\n",
      "|   588|[[4941, 8.71675],...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userSubsetRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(userSubsetRecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                                            |\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|300   |[[1295, 11.540114], [86345, 10.56369], [69406, 9.61298], [7669, 9.602613], [8810, 9.556323], [1301, 9.492831], [92535, 9.4679985], [4158, 9.454541], [1014, 9.376625], [3943, 9.238637]]   |\n",
      "|127   |[[7028, 17.53144], [3503, 16.630356], [938, 15.093879], [611, 14.892273], [3846, 14.721819], [1904, 14.521244], [46965, 14.173786], [2942, 14.028612], [5055, 13.968603], [3272, 13.53122]]|\n",
      "|151   |[[3272, 7.715576], [26865, 7.105867], [1295, 7.047054], [909, 6.617946], [1183, 6.6091228], [92535, 6.512807], [55444, 6.3292313], [2648, 6.254737], [2867, 6.222571], [166461, 6.1674585]]|\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userSubsetRecs.filter (\"userID in( 127, 151,  300)\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
